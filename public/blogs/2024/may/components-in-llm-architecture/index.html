<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta http-equiv="Accept-CH" content="DPR, Viewport-Width, Width">
<link rel="icon" href=/fav.png type="image/gif">


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
      as="style"
      href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
>
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
      media="print" onload="this.media='all'" />
<noscript>
  <link
          href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
          rel="stylesheet">
</noscript>


<link rel="stylesheet" href="/css/font.css" media="all">



<meta property="og:url" content="http://localhost:1313/blogs/2024/may/components-in-llm-architecture/">
  <meta property="og:site_name" content="Mohit Kanwar&#39;s App : My Cents">
  <meta property="og:title" content="Components in Llm Architecture">
  <meta property="og:description" content="Struggling to understand how machines create human-like text? The Transformer model is a game-changer in Natural Language Processing (NLP), but its inner workings can seem complex. This blog post breaks down the Transformer into easy-to-understand concepts, using clear language and avoiding technical jargon.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2024-05-14T09:28:50+05:30">
    <meta property="article:modified_time" content="2024-05-14T09:28:50+05:30">
    <meta property="article:tag" content="Artificial Intelligence">
    <meta property="article:tag" content="Transformer">
    <meta property="article:tag" content="Large Language Model">
    <meta property="article:tag" content="Architecture">
    <meta property="article:tag" content="Attention Is All You Need">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Components in Llm Architecture">
  <meta name="twitter:description" content="Struggling to understand how machines create human-like text? The Transformer model is a game-changer in Natural Language Processing (NLP), but its inner workings can seem complex. This blog post breaks down the Transformer into easy-to-understand concepts, using clear language and avoiding technical jargon.">


<link rel="stylesheet" href="/bootstrap-5/css/bootstrap.min.css" media="all"><link rel="stylesheet" href="/css/header.css" media="all">
<link rel="stylesheet" href="/css/footer.css" media="all">


<link rel="stylesheet" href="/css/theme.css" media="all">

<style>
    :root {
        --text-color: #343a40;
        --text-secondary-color: #6c757d;
        --text-link-color: #007bff;
        --background-color: #eaedf0;
        --secondary-background-color: #64ffda1a;
        --primary-color: #007bff;
        --secondary-color: #f8f9fa;

         
        --text-color-dark: #e4e6eb;
        --text-secondary-color-dark: #b0b3b8;
        --text-link-color-dark: #ffffff;
        --background-color-dark: #18191a;
        --secondary-background-color-dark: #212529;
        --primary-color-dark: #ffffff;
        --secondary-color-dark: #212529;
    }
    body {
        font-size: 1rem;
        font-weight: 400;
        line-height: 1.5;
        text-align: left;
    }

    html {
        background-color: var(--background-color) !important;
    }

    body::-webkit-scrollbar {
        height: 0px;
        width: 8px;
        background-color: var(--background-color);
    }

    ::-webkit-scrollbar-track {
        border-radius: 1rem;
    }

    ::-webkit-scrollbar-thumb {
        border-radius: 1rem;
        background: #b0b0b0;
        outline: 1px solid var(--background-color);
    }

    #search-content::-webkit-scrollbar {
        width: .5em;
        height: .1em;
        background-color: var(--background-color);
    }
</style>



<meta name="description" content="Struggling to understand how machines create human-like text? The Transformer model is a game-changer in Natural Language Processing (NLP), but its inner workings can seem complex. This blog post breaks down the Transformer into easy-to-understand concepts, using clear language and avoiding technical jargon.">
<link rel="stylesheet" href="/css/single.css">


<script defer src="/fontawesome-6/all-6.4.2.js"></script>


  
  

  <title>
Components in Llm Architecture | Mohit Kanwar&#39;s App : My Cents

  </title>
</head>

<body class="light">
  
  
<script>
    let localStorageValue = localStorage.getItem("pref-theme");
    let mediaQuery = window.matchMedia('(prefers-color-scheme: dark)').matches;

    switch (localStorageValue) {
        case "dark":
            document.body.classList.add('dark');
            break;
        case "light":
            document.body.classList.remove('dark');
            break;
        default:
            if (mediaQuery) {
                document.body.classList.add('dark');
            }
            break;
    }
</script>




<script>
    var prevScrollPos = window.pageYOffset;
    window.addEventListener("scroll", function showHeaderOnScroll() {
        let profileHeaderElem = document.getElementById("profileHeader");
        let currentScrollPos = window.pageYOffset;
        let resetHeaderStyle = false;
        let showNavBarOnScrollUp =  true ;
        let showNavBar = showNavBarOnScrollUp ? prevScrollPos > currentScrollPos : currentScrollPos > 0;
        if (showNavBar) {
            profileHeaderElem.classList.add("showHeaderOnTop");
        } else {
            resetHeaderStyle = true;
        }
        if(currentScrollPos === 0) {
            resetHeaderStyle = true;
        }
        if(resetHeaderStyle) {
            profileHeaderElem.classList.remove("showHeaderOnTop");
        }
        prevScrollPos = currentScrollPos;
    });
</script>



<header id="profileHeader">
    <nav class="pt-3 navbar navbar-expand-lg ">
        <div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5">
            
            <a class="navbar-brand primary-font text-wrap" href="/">
                
                <img src="/fav.png" width="30" height="30"
                    class="d-inline-block align-top">
                Mohit Kanwar
                
            </a>

            
                <div>
                    <input id="search" autocomplete="off" class="form-control mr-sm-2 d-none d-md-block" placeholder='Ctrl &#43; k to Search...'
                        aria-label="Search" oninput="searchOnChange(event)">
                </div>
            

            
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarContent"
                aria-controls="navbarContent" aria-expanded="false" aria-label="Toggle navigation">
                <svg aria-hidden="true" height="24" viewBox="0 0 16 16" version="1.1" width="24" data-view-component="true">
                    <path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path>
                </svg>
            </button>

            
            <div class="collapse navbar-collapse text-wrap primary-font" id="navbarContent">
                <ul class="navbar-nav ms-auto text-center">
                    
                        <li class="nav-item navbar-text d-block d-md-none">
                            <div class="nav-link">
                                <input id="search" autocomplete="off" class="form-control mr-sm-2" placeholder='Ctrl &#43; k to Search...' aria-label="Search" oninput="searchOnChange(event)">
                            </div>
                        </li>
                    

                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/#about" aria-label="about">
                            About Me
                        </a>
                    </li>
                    

                    

                    

                    

                    

                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/#contact"
                            aria-label="contact">
                            Contact
                        </a>
                    </li>
                    

                    

                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/blogs" title="Blog posts">
                            
                            Blogs
                        </a>
                    </li>
                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/solutions" title="Solutions">
                            
                            Solutions
                        </a>
                    </li>
                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/gallery" title="Blog posts">
                            
                            Gallery
                        </a>
                    </li>
                    
                    

                    
                    <li class="nav-item navbar-text">
                        
                        <div class="text-center">
                            <button id="theme-toggle">
                                <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                                </svg>
                                <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                    <circle cx="12" cy="12" r="5"></circle>
                                    <line x1="12" y1="1" x2="12" y2="3"></line>
                                    <line x1="12" y1="21" x2="12" y2="23"></line>
                                    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                                    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                                    <line x1="1" y1="12" x2="3" y2="12"></line>
                                    <line x1="21" y1="12" x2="23" y2="12"></line>
                                    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                                    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                                </svg>
                            </button>
                        </div>
                    </li>
                    

                </ul>

            </div>
        </div>
    </nav>
</header>
<div id="content">
<section id="single">
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-sm-12 col-md-12 col-lg-9">
        <div class="pr-lg-4">
          <div class="title mb-5">
            <h1 class="text-center mb-4">Components in Llm Architecture</h1>
            <div class="text-center">
              
                Mohit Kanwar
                <small>|</small>
              
              May 14, 2024

              
              <span id="readingTime">
                min read
              </span>
              
            </div>
          </div>
          
          <div class="featured-image">
            <img class="img-fluid mx-auto d-block" src="/images/blogs/2024/may/components-in-llm-architecture/banner.png" alt="Components in Llm Architecture">
          </div>
          
          <article class="page-content  p-2">
          <h1 id="transformer-model">Transformer Model</h1>
<p>Have you ever gotten frustrated with chatbots that just don&rsquo;t seem to understand the full flow of conversation? Or maybe you&rsquo;ve noticed machine translations that miss the mark completely. Well, there&rsquo;s a reason for that –  until recently, computers had a tough time understanding the deeper connections between words in a sentence. But a game-changer called the Transformer model is here!</p>
<p>The Transformer model is a deep learning architecture introduced in the paper <a href="https://arxiv.org/pdf/1706.03762">&ldquo;Attention is All You Need&rdquo;</a>. It revolutionized the field of natural language processing (NLP) by introducing a novel mechanism based on attention rather than recurrent or convolutional layers.</p>
<p>Attention mechanism allows the model to focus on important words and generate contextual meaning from them.</p>
<h1 id="text-generation-before-transformer">Text generation before transformer</h1>
<p>In the early days of text generation, computers relied on a technique called Recurrent Neural Networks (RNNs) to churn out sentences word by word. Imagine a student answering a series of questions, only able to consider the answer they just provided. RNNs worked similarly, predicting the next word based on the few words that came before it. This approach worked well for short sentences, like composing basic greetings or short commands. However, as sentences grew longer and more complex, RNNs started to stumble.</p>
<p>The problem arose because RNNs relied heavily on a concept called &ldquo;memory.&rdquo; They needed to remember the context of earlier words in the sequence to predict the upcoming ones effectively. But with limited computational power at the time, this memory became unreliable. The context of earlier words would either fade away entirely (vanishing gradients) or become disproportionately magnified (exploding gradients), making it difficult for the RNN to accurately predict the next word and capture the overall meaning of the sentence.  Furthermore, RNNs inherently struggled to grasp long-range dependencies between words spread far apart in a sentence. This meant they could miss subtle connections and nuances that are crucial for understanding the true intent of the text. As a result, RNN-based text generation often produced grammatically correct but nonsensical outputs, lacking the flow and coherence of natural language.</p>
<h1 id="transformer-architecture">Transformer Architecture</h1>
<p>The below architecture diagram is picked from the mentioned paper above.</p>
<img src="/images/blogs/2024/may/components-in-llm-architecture/architecture.webp" >
<p>The architecture consists of the following components :</p>
<h2 id="tokenizer--chopping-up-the-text-for-the-llm">Tokenizer : Chopping up the text for the LLM</h2>
<p>Imagine you&rsquo;re giving instructions to a machine that only understands basic building blocks. To make things clear, you wouldn&rsquo;t just dump the entire instruction at once, right? You&rsquo;d break it down into smaller, easier-to-understand pieces. That&rsquo;s exactly what a tokenizer does for a Large Language Model (LLM).</p>
<p>A tokenizer acts like a special tool that takes a sentence and chops it up into smaller units called tokens. These tokens can be words, punctuation marks, or even smaller pieces like letters (depending on the specific LLM and task).  Just like you wouldn&rsquo;t ask your machine to understand a whole sentence at once, the LLM can&rsquo;t process a giant block of text either. Tokens make the information manageable for the LLM, allowing it to work more efficiently.</p>
<p>Here&rsquo;s a deeper dive into how tokenizers function:</p>
<p><strong>Vocabulary</strong>: LLMs have a defined vocabulary, like a shortlist of words they understand. The tokenizer checks each piece of text against this vocabulary and breaks it down into the corresponding tokens.
<strong>Going Beyond Words</strong>: Sometimes, tokenizers don&rsquo;t just split text into words. For some tasks, they might separate punctuation marks or even break words down into characters. This depends on the specific needs of the LLM and the situation.
<strong>Adding Special Tokens</strong>: Think of special tokens like helper words for the LLM. The tokenizer might add these tokens to the beginning or end of a sentence to signal the start or finish, or to indicate special cases like unknown words.
By breaking down text into tokens, the tokenizer plays a crucial role in preparing information for the LLM. It ensures the LLM gets bite-sized pieces it can understand and work with, paving the way for accurate and efficient language processing.</p>
<h2 id="embeddings--turning-tokens-into-meaningful-codes">Embeddings : Turning Tokens into Meaningful Codes</h2>
<p>Imagine you have a giant dictionary where every word has a unique code assigned to it. But this code isn&rsquo;t just a random number – it captures the essence of the word, its meaning, and how it relates to other words. That&rsquo;s exactly what embeddings do for tokens in the Transformer model.</p>
<p>Embeddings are like secret codes, but instead of words, they represent tokens generated by the tokenizer. Each token gets converted into a vector, which is basically a fancy way of saying a list of numbers. The cool part is that these numbers aren&rsquo;t random – they&rsquo;re carefully chosen by the Transformer model during training to reflect the token&rsquo;s meaning and its connection to other tokens.</p>
<p>Here&rsquo;s how embeddings capture meaning:</p>
<p><strong>Similar Tokens, Similar Codes</strong>: In the embedding space (imagine a giant map where each point represents a token&rsquo;s code), tokens with similar meanings are positioned closer together. For example, &ldquo;king&rdquo; and &ldquo;queen&rdquo; would likely have codes that are close neighbors in this space, while &ldquo;king&rdquo; and &ldquo;key&rdquo; would be much farther apart.
<strong>Context is Key (For Advanced Models)</strong>: In simpler models, embeddings might just represent the general meaning of a token. But advanced models take things a step further. They can generate contextual embeddings, which means the code for a token can shift slightly depending on the surrounding words. For instance, the embedding for &ldquo;play&rdquo; in the sentence &ldquo;Let&rsquo;s play outside&rdquo; might be different from the embedding for &ldquo;play&rdquo; in the sentence &ldquo;Press play on the music.&rdquo;
By converting tokens into embeddings, the Transformer model goes beyond just recognizing individual words. It creates a meaningful representation of language, allowing it to understand the relationships between words and generate more natural and coherent text.</p>
<h2 id="the-encoder-unveiling-the-hidden-connections">The Encoder: Unveiling the Hidden Connections</h2>
<p>Imagine you&rsquo;re reading a detective story. To understand who committed the crime, you need to analyze all the clues, not just the ones that appear right next to each other. The Transformer&rsquo;s encoder works in a similar way. It takes the token embeddings, those secret codes packed with meaning, and analyzes them all together to uncover the hidden connections and understand the overall context of the text.</p>
<p>The encoder is like a master detective board. It lays out all the embeddings (clues) and uses a special technique called self-attention to see how each piece connects to the others.  Self-attention allows the encoder to focus on relevant parts of the sentence, even if they&rsquo;re far apart.  Here&rsquo;s how it works:</p>
<p><strong>Beyond Neighbors</strong>: Unlike reading words one by one, the encoder can consider every token&rsquo;s embedding at the same time. This lets it capture long-range dependencies – connections between words that might be separated by many other words in the sentence. For example, the encoder can understand how the subject of a sentence relates to the verb, even if they&rsquo;re far apart.
<strong>Not All Clues Are Equal</strong>: The encoder doesn&rsquo;t just blindly analyze every connection. It assigns a score to each relationship, indicating how important it is for understanding the meaning. This ensures the encoder focuses on the most relevant clues, just like a detective prioritizing the most critical pieces of evidence.</p>
<p>Encoders are typically layered, with each layer building on the work of the previous one. The first layer might identify basic connections, and subsequent layers refine these connections and uncover deeper contextual information. By meticulously examining all the embeddings and their relationships, the encoder paints a clear picture of the meaning behind the words, preparing the ground for the next stage – generating a response.</p>
<h2 id="the-decoder-weaving-a-response-from-the-unraveled-context">The Decoder: Weaving a Response from the Unraveled Context</h2>
<p>The encoder, like a skilled detective, has meticulously analyzed the text and uncovered its hidden meaning. But what if that meaning just sits there, unutilized? That&rsquo;s where the decoder comes in.  Imagine the decoder as a creative writer who takes the detective&rsquo;s findings (the encoder&rsquo;s output) and uses them to craft a response, like a new sentence, translation, or summary.</p>
<p>Similar to the encoder, the decoder is layered. Each layer takes the output from the previous layer, which is a refined understanding of the context, and builds upon it. Here&rsquo;s how the decoder works step-by-step:</p>
<p><strong>Starting Point</strong>: The decoder typically begins with a special start token, signaling the beginning of the response it&rsquo;s about to create.
<strong>Attention to the Context</strong>: Just like the encoder, the decoder utilizes attention mechanisms. But instead of focusing on all the token embeddings at once, it pays attention to the encoder&rsquo;s output and the previously generated tokens in the response (if it&rsquo;s a multi-word response). This allows the decoder to consider the overall context and ensure the response aligns with the original meaning.
<strong>Word by Word Creation</strong>: At each step, the decoder predicts the next token in the response. It considers the previously generated tokens, the encoder&rsquo;s output (the context), and its own internal knowledge to pick the most fitting word.
<strong>Building Up the Response</strong>: With each prediction, the decoder adds a new token to the response, one by one. This iterative process continues until the decoder predicts an end token, signifying the completion of the response.</p>
<p>The type of decoder used can vary depending on the desired outcome. For tasks like generating full sentences, a different decoder architecture might be used compared to tasks that require summaries. But ultimately, all decoders share the same core function: transforming the encoder&rsquo;s contextual understanding into a meaningful response.  By working together, the encoder and decoder form the heart of the Transformer model, allowing it to understand and respond to language in a way that surpasses traditional methods.</p>
<h2 id="softmax-picking-the-perfect-word-with-probability">SoftMax: Picking the Perfect Word (with Probability!)</h2>
<p>Imagine you&rsquo;ve narrowed down your choices to the final few words for your response, but you&rsquo;re not quite sure which one lands the perfect punch. That&rsquo;s where SoftMax steps in. It acts like a probability judge, analyzing the remaining candidate tokens (words) and picking the one with the highest chance of being the most fitting choice.</p>
<p>SoftMax isn&rsquo;t a simple on/off switch. It considers the probability of each token and assigns a value between 0 and 1.  Think of it like a confidence score – a higher score indicates a greater chance of being the ideal word for the situation. The magic lies in how SoftMax treats these probabilities:</p>
<p><strong>Normalization is Key</strong>: SoftMax takes all the probabilities and adjusts them proportionally, ensuring they all add up to 1. This &ldquo;normalization&rdquo; prevents any single token from dominating the selection process unfairly.
<strong>The Soft in SoftMax</strong>: By normalizing the probabilities, SoftMax avoids picking the absolute highest value every time. This &ldquo;softness&rdquo; allows for some flexibility, considering not just the most likely option but also giving some weight to other possibilities with decent probabilities.
So, how does SoftMax use these probabilities to pick a word? It performs a weighted random selection based on the assigned probabilities.  Think of it like a lottery where each token has a number of tickets corresponding to its probability score. The higher the score, the more tickets a token has in the basket. SoftMax randomly picks a ticket, and the word on that ticket becomes the chosen token for the response.</p>
<p>SoftMax plays a crucial role in ensuring the Transformer model generates the most relevant and fitting words for the situation. It takes the possibilities presented by the decoder and selects the one with the highest chance of success, adding a layer of refinement and making the Transformer&rsquo;s responses more natural and accurate.</p>

          </article>
        </div>
      </div>
      <div class="col-sm-12 col-md-12 col-lg-3">
        <div id="stickySideBar" class="sticky-sidebar">
          
          <aside class="toc">
              <h5>
                Table Of Contents
              </h5>
              <div class="toc-content">
                <nav id="TableOfContents">
  <ul>
    <li><a href="#tokenizer--chopping-up-the-text-for-the-llm">Tokenizer : Chopping up the text for the LLM</a></li>
    <li><a href="#embeddings--turning-tokens-into-meaningful-codes">Embeddings : Turning Tokens into Meaningful Codes</a></li>
    <li><a href="#the-encoder-unveiling-the-hidden-connections">The Encoder: Unveiling the Hidden Connections</a></li>
    <li><a href="#the-decoder-weaving-a-response-from-the-unraveled-context">The Decoder: Weaving a Response from the Unraveled Context</a></li>
    <li><a href="#softmax-picking-the-perfect-word-with-probability">SoftMax: Picking the Perfect Word (with Probability!)</a></li>
  </ul>
</nav>
              </div>
          </aside>
          

          
          <aside class="tags">
            <h5>Tags</h5>
            <ul class="tags-ul list-unstyled list-inline">
              
              <li class="list-inline-item"><a href="http://localhost:1313/tags/artificial-intelligence"
                target="_blank"
              >Artificial intelligence</a></li>
              
              <li class="list-inline-item"><a href="http://localhost:1313/tags/transformer"
                target="_blank"
              >Transformer</a></li>
              
              <li class="list-inline-item"><a href="http://localhost:1313/tags/large-language-model"
                target="_blank"
              >Large Language Model</a></li>
              
              <li class="list-inline-item"><a href="http://localhost:1313/tags/architecture"
                target="_blank"
              >Architecture</a></li>
              
              <li class="list-inline-item"><a href="http://localhost:1313/tags/attention-is-all-you-need"
                target="_blank"
              >Attention is all you need</a></li>
              
            </ul>
          </aside>
          

          
          <aside class="social">
            <h5>Social</h5>
            <div class="social-content">
              <ul class="list-inline">
                <li class="list-inline-item text-center">
                  <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2flocalhost%3a1313%2fblogs%2f2024%2fmay%2fcomponents-in-llm-architecture%2f">
                    <i class="fab fa-linkedin"></i>
                  </a>
                </li>
                <li class="list-inline-item text-center">
                  <a target="_blank" href="https://twitter.com/share?text=Components%20in%20Llm%20Architecture&url=http%3a%2f%2flocalhost%3a1313%2fblogs%2f2024%2fmay%2fcomponents-in-llm-architecture%2f">
                    <i class="fab fa-twitter"></i>
                  </a>
                </li>
                <li class="list-inline-item text-center">
                  <a target="_blank" href="https://api.whatsapp.com/send?text=Components%20in%20Llm%20Architecture: http%3a%2f%2flocalhost%3a1313%2fblogs%2f2024%2fmay%2fcomponents-in-llm-architecture%2f">
                    <i class="fab fa-whatsapp"></i>
                  </a>
                </li>
                <li class="list-inline-item text-center">
                  <a target="_blank" href='mailto:?subject=Components%20in%20Llm%20Architecture&amp;body=Check%20out%20this%20site http%3a%2f%2flocalhost%3a1313%2fblogs%2f2024%2fmay%2fcomponents-in-llm-architecture%2f'>
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
              </ul>
            </div>
          </aside>
          
        </div>
      </div>
    </div>
    <div class="row">
      <div class="col-sm-12 col-md-12 col-lg-9 p-4">
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mohitkanwar-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
    </div>
  </div>
  <button class="p-2 px-3" onclick="topFunction()" id="topScroll">
    <i class="fas fa-angle-up"></i>
  </button>
</section>


<div class="progress">
  <div id="scroll-progress-bar" class="progress-bar" role="progressbar" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100"></div>
</div>
<Script src="/js/scrollProgressBar.js"></script>


<script>
  var topScroll = document.getElementById("topScroll");
  window.onscroll = function() {scrollFunction()};

  function scrollFunction() {
    if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
      topScroll.style.display = "block";
    } else {
      topScroll.style.display = "none";
    }
  }

  function topFunction() {
    document.body.scrollTop = 0;
    document.documentElement.scrollTop = 0;
  }

  
  let stickySideBarElem = document.getElementById("stickySideBar");
  let stickyNavBar =  true ;
  if(stickyNavBar) {
    let headerElem = document.getElementById("profileHeader");
    let headerHeight = headerElem.offsetHeight + 15;
    stickySideBarElem.style.top = headerHeight + "px";
  } else {
    stickySideBarElem.style.top = "50px";
  }
</script>


<script src="/js/readingTime.js"></script>



  </div><footer>
    
 
 
<div class="container py-3" id="recent-posts">
    
    
    <div class="h3 text-center text-secondary py-3">
        Recent Posts
    </div>
    <div class="row justify-content-center">
        
        <div class="col-lg-4 col-md-6 pt-2">
            <div class="card h-100">
                
                <div class="card-header">
                    <a href="/blogs/2025/jul/pins-and-passwords/">
                        <img src="/images/blogs/2025/jan/the-test-data-dilemma/banner.png" class="card-img-top" alt="Pins and Passwords">
                    </a>
                </div>
                
                <div class="card-body bg-transparent p-3 shadow-sm">
                    <a href="/blogs/2025/jul/pins-and-passwords/" class="primary-font card-title">
                        <h5 class="card-title bg-transparent" title="Pins and Passwords">Pins and Passwords</h5>
                    </a>
                    <div class="card-text secondary-font">
                        <p><h1 id="introduction">Introduction</h1>
<p>How many of us have stared blankly at a login screen, desperately trying to recall that complex password? It’s a frustratingly common experience, and a surprisingly significant hurdle in securing mobile applications. During my architecture design consulting sessions, I come across this …</p></p>
                    </div>
                </div>
                <div class="mt-auto card-footer">
                    <span class="float-start">Jul 1, 2025</span>
                    <a href="/blogs/2025/jul/pins-and-passwords/" class="float-end btn btn-outline-info btn-sm">Read</a>
                </div>
            </div>
        </div>
        
        <div class="col-lg-4 col-md-6 pt-2">
            <div class="card h-100">
                
                <div class="card-header">
                    <a href="/blogs/2025/jan/the-test-data-dilemma/">
                        <img src="/images/blogs/2025/jan/the-test-data-dilemma/banner.png" class="card-img-top" alt="The Test Data Dilemma">
                    </a>
                </div>
                
                <div class="card-body bg-transparent p-3 shadow-sm">
                    <a href="/blogs/2025/jan/the-test-data-dilemma/" class="primary-font card-title">
                        <h5 class="card-title bg-transparent" title="The Test Data Dilemma">The Test Data Dilemma</h5>
                    </a>
                    <div class="card-text secondary-font">
                        <p><h3 id="the-test-data-dilemma-a-silent-challenge-in-software-development">The Test Data Dilemma: A Silent Challenge in Software Development</h3>
<p>When building software, one big challenge often goes unnoticed – getting the right test data. As a software architect, I have worked with many customers to create software systems. A common issue I’ve faced is that customers don’t …</p></p>
                    </div>
                </div>
                <div class="mt-auto card-footer">
                    <span class="float-start">Jan 6, 2025</span>
                    <a href="/blogs/2025/jan/the-test-data-dilemma/" class="float-end btn btn-outline-info btn-sm">Read</a>
                </div>
            </div>
        </div>
        
        <div class="col-lg-4 col-md-6 pt-2">
            <div class="card h-100">
                
                <div class="card-header">
                    <a href="/blogs/2024/nov/designing-a-captcha-service/">
                        <img src="/images/blogs/2024/nov/designing-a-captcha-service/banner.png" class="card-img-top" alt="Designing a Self-Hosted Captcha Service: Challenges, Insights, and Solutions">
                    </a>
                </div>
                
                <div class="card-body bg-transparent p-3 shadow-sm">
                    <a href="/blogs/2024/nov/designing-a-captcha-service/" class="primary-font card-title">
                        <h5 class="card-title bg-transparent" title="Designing a Self-Hosted Captcha Service: Challenges, Insights, and Solutions">Designing a Self-Hosted …</h5>
                    </a>
                    <div class="card-text secondary-font">
                        <p><p><strong>Introduction</strong>: What if the familiar captcha boxes we encounter daily were designed by your team, tailored specifically to your organization’s needs?<br>
Recently, I took on the challenge of creating a custom, self-hosted captcha service for a client. It sounded simple at first—why reinvent the wheel when …</p></p>
                    </div>
                </div>
                <div class="mt-auto card-footer">
                    <span class="float-start">Nov 1, 2024</span>
                    <a href="/blogs/2024/nov/designing-a-captcha-service/" class="float-end btn btn-outline-info btn-sm">Read</a>
                </div>
            </div>
        </div>
        
    </div>
</div>

<div class="text-center pt-2">
    
    <span class="px-1">
        <a href="https://github.com/mohitkanwar" aria-label="github">
            <svg xmlns="http://www.w3.org/2000/svg" width="2.7em" height="2.7em" viewBox="0 0 1792 1792">
                <path id="footer-socialNetworks-github-svg-path"
                    d="M522 1352q-8 9-20-3-13-11-4-19 8-9 20 3 12 11 4 19zm-42-61q9 12 0 19-8 6-17-7t0-18q9-7 17 6zm-61-60q-5 7-13 2-10-5-7-12 3-5 13-2 10 5 7 12zm31 34q-6 7-16-3-9-11-2-16 6-6 16 3 9 11 2 16zm129 112q-4 12-19 6-17-4-13-15t19-7q16 5 13 16zm63 5q0 11-16 11-17 2-17-11 0-11 16-11 17-2 17 11zm58-10q2 10-14 14t-18-8 14-15q16-2 18 9zm964-956v960q0 119-84.5 203.5t-203.5 84.5h-224q-16 0-24.5-1t-19.5-5-16-14.5-5-27.5v-239q0-97-52-142 57-6 102.5-18t94-39 81-66.5 53-105 20.5-150.5q0-121-79-206 37-91-8-204-28-9-81 11t-92 44l-38 24q-93-26-192-26t-192 26q-16-11-42.5-27t-83.5-38.5-86-13.5q-44 113-7 204-79 85-79 206 0 85 20.5 150t52.5 105 80.5 67 94 39 102.5 18q-40 36-49 103-21 10-45 15t-57 5-65.5-21.5-55.5-62.5q-19-32-48.5-52t-49.5-24l-20-3q-21 0-29 4.5t-5 11.5 9 14 13 12l7 5q22 10 43.5 38t31.5 51l10 23q13 38 44 61.5t67 30 69.5 7 55.5-3.5l23-4q0 38 .5 103t.5 68q0 22-11 33.5t-22 13-33 1.5h-224q-119 0-203.5-84.5t-84.5-203.5v-960q0-119 84.5-203.5t203.5-84.5h960q119 0 203.5 84.5t84.5 203.5z" />

                <metadata>
                    <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
                        xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" xmlns:dc="http://purl.org/dc/elements/1.1/">
                        <rdf:Description about="https://iconscout.com/legal#licenses"
                            dc:title="Github, Online, Project, Hosting, Square"
                            dc:description="Github, Online, Project, Hosting, Square" dc:publisher="Iconscout"
                            dc:date="2016-12-14" dc:format="image/svg+xml" dc:language="en">
                            <dc:creator>
                                <rdf:Bag>
                                    <rdf:li>Font Awesome</rdf:li>
                                </rdf:Bag>
                            </dc:creator>
                        </rdf:Description>
                    </rdf:RDF>
                </metadata>
            </svg>
        </a>
    </span>
    

    
    <span class="px-1">
        <a href="https://www.linkedin.com/in/mohit-kanwar-7668a211/" aria-label="linkedin">
            <svg xmlns="http://www.w3.org/2000/svg" width='2.4em' height='2.4em' fill="#fff" aria-label="LinkedIn"
                viewBox="0 0 512 512">
                <rect width="512" height="512" fill="#0077b5" rx="15%" />
                <circle cx="142" cy="138" r="37" />
                <path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198" />
                <path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
        </a>
    </span>
    

    
    <a href="https://twitter.com/mmkanwar" aria-label="twitter">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 48 48" width="48px" height="48px">
            <path fill="#03a9f4"
                d="M42,37c0,2.762-2.239,5-5,5H11c-2.762,0-5-2.238-5-5V11c0-2.762,2.238-5,5-5h26c2.761,0,5,2.238,5,5 V37z" />
            <path fill="#fff"
                d="M36,17.12c-0.882,0.391-1.999,0.758-3,0.88c1.018-0.604,2.633-1.862,3-3 c-0.951,0.559-2.671,1.156-3.793,1.372C31.311,15.422,30.033,15,28.617,15C25.897,15,24,17.305,24,20v2c-4,0-7.9-3.047-10.327-6 c-0.427,0.721-0.667,1.565-0.667,2.457c0,1.819,1.671,3.665,2.994,4.543c-0.807-0.025-2.335-0.641-3-1c0,0.016,0,0.036,0,0.057 c0,2.367,1.661,3.974,3.912,4.422C16.501,26.592,16,27,14.072,27c0.626,1.935,3.773,2.958,5.928,3c-1.686,1.307-4.692,2-7,2 c-0.399,0-0.615,0.022-1-0.023C14.178,33.357,17.22,34,20,34c9.057,0,14-6.918,14-13.37c0-0.212-0.007-0.922-0.018-1.13 C34.95,18.818,35.342,18.104,36,17.12" />
        </svg>
    </a>
    

    

    
</div><div class="container py-4">
    <div class="row justify-content-center">
        <div class="col-md-4 text-center">
            
                <div class="pb-2">
                    <a href="http://localhost:1313/" title="Mohit Kanwar&#39;s App : My Cents">
                        <img alt="Footer logo" src="/fav.png"
                            height="40px" width="40px">
                    </a>
                </div>
            
            &copy; 2025  All rights reserved
            <div class="text-secondary">
                Made with
                <span class="text-danger">
                    &#10084;
                </span>
                and
                <a href="https://github.com/gurusabarish/hugo-profile" target="_blank"
                    title="Designed and developed by gurusabarish">
                    Hugo Profile
                </a>
            </div>
        </div>
    </div>
</div>
</footer><script src="/bootstrap-5/js/bootstrap.bundle.min.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

    var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'))
    var tooltipList = tooltipTriggerList.map(function (tooltipTriggerEl) {
        return new bootstrap.Tooltip(tooltipTriggerEl)
    })

</script>


    <script src="/js/search.js"></script>











  <section id="search-content" class="py-2">
    <div class="container" id="search-results"></div>
  </section>
</body>

</html>
